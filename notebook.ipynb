{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Add the path to stylegan2-ada-pytorch directory\n",
    "repo_path = '/Users/oliverlundin/Local Documents/github/facemorph/stylegan2-ada-pytorch'\n",
    "sys.path.append(repo_path)\n",
    "\n",
    "import projector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face1_path = 'aligned/align-IMG_0925.jpeg'\n",
    "# face2_path = 'aligned/align-IMG_5267.jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images to morph\n",
    "\n",
    "face1_path = 'images/anna.jpg'\n",
    "face2_path = 'images/filip.jpg'\n",
    "\n",
    "# face1_path = 'images/emma.jpg'\n",
    "# face2_path = 'images/oliver.jpg'\n",
    "\n",
    "# face1_path = 'images/vilma.jpg'\n",
    "# face2_path = 'images/emma.jpg'\n",
    "\n",
    "# face1_path = 'images/vilma.jpg'\n",
    "# face2_path = 'images/oliver.jpg'\n",
    "\n",
    "# face1_path = 'images/pier.jpg'\n",
    "# face2_path = 'images/tom.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images and convert to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from utils import align_face\n",
    "import numpy as np\n",
    "\n",
    "face1 = Image.open(face1_path)\n",
    "face2 = Image.open(face2_path)\n",
    "face1 = align_face(face1)\n",
    "face2 = align_face(face2)\n",
    "\n",
    "\n",
    "# Convert the PIL images to NumPy arrays\n",
    "face1_array = np.array(face1)\n",
    "face2_array = np.array(face2)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors with specified dtype\n",
    "face1_tensor = torch.tensor(face1_array, dtype=torch.float16)  # Change dtype as needed\n",
    "face2_tensor = torch.tensor(face2_array, dtype=torch.float16)\n",
    "\n",
    "# Print the shape of the tensors\n",
    "print(face1_tensor.shape)\n",
    "print(face2_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if we can use GPU, MPS for mac CUDA for nvidia and load the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set device\n",
    "if(torch.backends.mps.is_available()): # True\n",
    "    print(\"MPS is available\")\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "# Load the generator model from the pickle file\n",
    "with open('ffhq_res256.pkl', 'rb') as f:\n",
    "\tG = pickle.load(f)['G_ema'].to(device) \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the image tensors to match the generators expected output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eftersom generatorn är tränad på en viss typ av bilder av en viss storlek måste våra bilder matcha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face1_tensor = face1_tensor.squeeze()\n",
    "face1_tensor = face1_tensor.permute(2, 0, 1)\n",
    "face1_tensor = F.interpolate(face1_tensor.unsqueeze(0), size=(G.img_resolution, G.img_resolution), mode='bilinear', align_corners=False)\n",
    "face1_tensor = face1_tensor.squeeze(0)\n",
    "\n",
    "face2_tensor = face2_tensor.squeeze()\n",
    "face2_tensor = face2_tensor.permute(2, 0, 1)\n",
    "face2_tensor = F.interpolate(face2_tensor.unsqueeze(0), size=(G.img_resolution, G.img_resolution), mode='bilinear', align_corners=False)\n",
    "face2_tensor = face2_tensor.squeeze(0)\n",
    "\n",
    "face1_tensor = face1_tensor.to(device)\n",
    "face2_tensor = face2_tensor.to(device)\n",
    "\n",
    "# empty cache\n",
    "torch.mps.empty_cache()\n",
    "\n",
    "# Debugging: Print the shapes\n",
    "print(f\"face1_tensor shape: {face1_tensor.shape}\")\n",
    "print(f\"face2_tensor shape: {face2_tensor.shape}\")\n",
    "print(f\"Expected shape: ({G.img_channels}, {G.img_resolution}, {G.img_resolution})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project first image into W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antalet steps är antalet steg den tar i W rymden för att hitta en bild som är så lik bilden vi gett den, fler steg blir alltså bättre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Project the image\n",
    "projected_w_steps1 = projector.project(\n",
    "    G,\n",
    "    target=face1_tensor,  # Your target image tensor\n",
    "    num_steps=2,  # Number of optimization steps\n",
    "\tdevice = device,\n",
    "    verbose=True  # Print optimization progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project second image into W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_w_steps2 = projector.project(\n",
    "    G,\n",
    "    target=face2_tensor,  # Your target image tensor\n",
    "    num_steps=200,  # Number of optimization steps\n",
    "    device=device,\n",
    "    verbose=True  # Print optimization progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_w_steps1.shape\n",
    "projected_w_steps2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the projected_w_steps1 and projected_w_steps2 are exactly the same\n",
    "(projected_w_steps1 == projected_w_steps2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hitta rätt matriser i projektionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = projected_w_steps1[-1].unsqueeze(0)\n",
    "w2 = projected_w_steps2[-1].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate first image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om man vill generera en \"ny\" bild utifrån den första bilden kan man köra nedanstående två celler. Men det är egentligen inte nödvändigt och man kan skippa dessa steg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = G.synthesis(w1, noise_mode='const', force_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = (img1 + 1) * (255/2)\n",
    "img1 = img1.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = G.synthesis(w2, noise_mode='const', force_fp32=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = (img2 + 1) * (255/2)\n",
    "img2 = img2.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate between w1 and w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolera mellan projektionerna, 10 steg ger 10 bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear interpolation between w1 and w2\n",
    "num_interpolations = 10\n",
    "interpolations = torch.zeros((num_interpolations, w1.shape[1], w1.shape[2]), device=device)\n",
    "for i in range(num_interpolations):\n",
    "\tinterpolations[i] = w1 + (w2 - w1) * i / (num_interpolations - 1)\n",
    "\n",
    "print(interpolations.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the interpolations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generera 10 bilder utifrån interpolationerna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the images\n",
    "interpolated_images = G.synthesis(interpolations, noise_mode='const', force_fp32=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interpolated_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolla olika bilder utifrån vilket index man stoppar i interpolated_images, 4/5 är någonstanns i mitten och är själva morphen. 0 är bild 1 och 9 är bild 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_images.shape\n",
    "interpolated_images[5].shape\n",
    "\n",
    "interpolated_images = (interpolated_images + 1) * (255/2)\n",
    "\n",
    "interpolated_images = interpolated_images.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "\n",
    "print(interpolated_images.shape)\n",
    "\n",
    "# plot the middle image\n",
    "plt.imshow(interpolated_images[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear the plot\n",
    "plt.clf()\n",
    "\n",
    "#insert face1array image in the begining and face2array image in the end\n",
    "interpolated_images = np.insert(interpolated_images, 0, face1_array, axis=0)\n",
    "interpolated_images = np.insert(interpolated_images, len(interpolated_images), face2_array, axis=0)\n",
    "\n",
    "num_images = len(interpolated_images)\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
    "\n",
    "if num_images == 1:\n",
    "\taxes = [axes]\n",
    "\n",
    "for i, img in enumerate(interpolated_images):\n",
    "\taxes[i].imshow(img)\n",
    "\taxes[i].axis('off')\n",
    "\taxes[i].set_frame_on(False)\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facemorphing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
